---
---

@inproceedings{DBLP:conf/wosp/HegemanJIT21,
  abbr        = {HotCloudPerf},
  bibtex_show = {true},
  selected    = {false},
  preview     = {grademl.png},
  pdf         = {https://atlarge-research.com/pdfs/hotcloudperf2021_GradeML.pdf},
  abstract    = {Today, machine learning (ML) workloads are nearly ubiquitous. Over the past decade, much effort has been put into making ML model-training fast and efficient, e.g., by proposing new ML frameworks (such as TensorFlow, PyTorch), leveraging hardware support (TPUs, GPUs, FPGAs), and implementing new execution models (pipelines, distributed training). Matching this trend, considerable effort has also been put into performance analysis tools focusing on ML model-training. However, as we identify in this work, ML model training rarely happens in isolation and is instead one step in a larger ML workflow. Therefore, it is surprising that there exists no performance analysis tool that covers the entire life-cycle of ML workflows. Addressing this large conceptual gap, we envision in this work a holistic performance analysis tool for ML workflows. We analyze the state-of-practice and the state-of-the-art, presenting quantitative evidence about the performance of existing performance tools. We formulate our vision for holistic performance analysis of ML workflows along four design pillars: a unified execution model, lightweight collection of performance data, efficient data aggregation and presentation, and close integration in ML systems. Finally, we propose first steps towards implementing our vision as GradeML, a holistic performance analysis tool for ML workflows. Our preliminary work and experiments are open source at https://github.com/atlarge-research/grademl},
  author      = {Tim Hegeman and
                 Matthijs Jansen and
                 Alexandru Iosup and
                 Animesh Trivedi},
  title       = {GradeML: Towards Holistic Performance Analysis for Machine Learning Workflows},
  booktitle   = {HotCloudPerf},
  year        = {2021},
  editor      = {Johann Bourcier and
                 Zhen Ming (Jack) Jiang and
                 Cor{-}Paul Bezemer and
                 Vittorio Cortellessa and
                 Daniele Di Pompeo and
                 Ana Lucia Varbanescu},
  pages       = {57--63},
  publisher   = {{ACM}},
  doi         = {10.1145/3447545.3451185},
  biburl      = {https://dblp.org/rec/conf/wosp/HegemanJIT21.bib},
  bibsource   = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/sc/JansenCV20,
  abbr        = {DLS@SC},
  bibtex_show = {true},
  selected    = {true},
  preview     = {ddlbench.png},
  pdf         = {https://atlarge-research.com/pdfs/jansen2020ddlbench.pdf},
  abstract    = {Due to its many applications across various fields of research, engineering, and daily life, deep learning has seen a surge in popularity. Therefore, larger and more expressive models have been proposed, with examples like Turing-NLG using as many as 17 billion parameters. Training these very large models becomes increasingly difficult due to the high computational costs and large memory footprint. Therefore, several approaches for distributed training based on data parallelism (e.g., Horovod) and model/pipeline parallelism (e.g., GPipe, PipeDream) have emerged. In this work, we focus on an in-depth comparison of three different parallelism models that address these needs: data, model and pipeline parallelism. To this end, we provide an analytical comparison of the three, both in terms of computation time and memory usage, and introduce DDLBench, a comprehensive (open-source, ready-to-use) benchmark suite to quantify these differences in practice. Through in-depth performance analysis and experimentation with various models, datasets, distribution models and hardware systems, we demonstrate that DDLBench can accurately quantify the capability of a given system to perform distributed deep learning (DDL). By comparing our analytical models with the benchmarking results, we show how the performance of real-life implementations diverges from these analytical models, thus requiring benchmarking to capture the in-depth complexity of the frameworks themselves.},
  author      = {Matthijs Jansen and
                 Valeriu Codreanu and
                 Ana Lucia Varbanescu},
  title       = {DDLBench: Towards a Scalable Benchmarking Infrastructure for Distributed Deep Learning},
  booktitle   = {Deep Learning on Supercomputers},
  pages       = {31--39},
  publisher   = {{IEEE}},
  year        = {2020},
  url         = {https://doi.org/10.1109/DLS51937.2020.00009},
  doi         = {10.1109/DLS51937.2020.00009},
  biburl      = {https://dblp.org/rec/conf/sc/JansenCV20.bib},
  bibsource   = {dblp computer science bibliography, https://dblp.org}
}
